import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from google.colab import drive
drive.mount('/content/drive')
file_path = '/content/drive/MyDrive/D3.csv'
sample = pd.DataFrame(pd.read_csv(file_path))
sample.head()


# Define the target variable ('Y' in your case)
target_variable = 'Y'

# Extract explanatory variables (X1, X2, X3)
explanatory_variables = ['X1', 'X2', 'X3']

# Function for gradient descent
def gradient_descent(X, y, learning_rate, num_iterations):
    m, n = X.shape
    theta = np.zeros((n, 1))
    cost_history = []

    for i in range(num_iterations):
        gradients = (1/m) * X.T.dot(X.dot(theta) - y)
        theta -= learning_rate * gradients
        cost = (1/(2*m)) * np.sum((X.dot(theta) - y)**2)
        cost_history.append(cost)

    return theta, cost_history

# Define hyperparameters
learning_rates = [0.1, 0.05, 0.01]  # You can explore different learning rates
num_iterations = 1000

# Initialize dictionaries to store results
results = {}

# Perform linear regression with gradient descent for each explanatory variable
for variable in explanatory_variables:
    X = sample[[variable]].values  # Extract the current explanatory variable
    y = sample[target_variable].values.reshape(-1, 1)

    # Add a bias term to the input features
    X_b = np.c_[np.ones((X.shape[0], 1)), X]

    # Initialize dictionaries for this variable
    variable_results = {"theta": [], "cost_history": []}

    # Perform gradient descent for each learning rate
    for learning_rate in learning_rates:
        theta, cost_history = gradient_descent(X_b, y, learning_rate, num_iterations)

        # Store results
        variable_results["theta"].append(theta)
        variable_results["cost_history"].append(cost_history)

    # Store variable-specific results
    results[variable] = variable_results

# a. Report the linear model for each explanatory variable
for variable, variable_results in results.items():
    print(f"Explanatory Variable: {variable}")
    for i, learning_rate in enumerate(learning_rates):
        print(f"Learning Rate: {learning_rate}")
        print("Linear Model:")
        print(f"Y = {variable_results['theta'][i][0][0]:.4f} + {variable_results['theta'][i][1][0]:.4f} * {variable}")
        print("\n")

# b. Plot the final regression model and loss over the iteration per each explanatory variable
plt.figure(figsize=(15, 5))
for i, variable in enumerate(explanatory_variables):
    plt.subplot(1, 3, i+1)
    plt.title(f'Regression Model (Variable: {variable})')
    plt.scatter(sample[variable], sample[target_variable], alpha=0.5, label='Data')
    for j, learning_rate in enumerate(learning_rates):
        plt.plot(sample[variable], results[variable]["theta"][j][0] + results[variable]["theta"][j][1] * sample[variable],
                 label=f'LR={learning_rate}', linewidth=2)
    plt.xlabel(variable)
    plt.ylabel(target_variable)
    plt.legend()

# c. Determine which explanatory variable has the lower loss (cost) for explaining the output (Y)
min_loss_variable = None
min_loss_value = float('inf')

for variable, variable_results in results.items():
    final_loss = variable_results["cost_history"][-1][-1]
    if final_loss < min_loss_value:
        min_loss_value = final_loss
        min_loss_variable = variable

print(f"c. Explanatory Variable with Lower Loss: {min_loss_variable} (Loss: {min_loss_value:.4f})")

# d. Describe the impact of different learning rates on the final loss and number of training iterations
for variable, variable_results in results.items():
    print(f"\nd. Impact of Learning Rates (Variable: {variable})")
    for i, learning_rate in enumerate(learning_rates):
        final_loss = variable_results["cost_history"][i][-1]
        num_iterations_used = len(variable_results["cost_history"][i])
        print(f"Learning Rate: {learning_rate}")
        print(f"Final Loss: {final_loss:.4f}")
        print(f"Number of Iterations: {num_iterations_used}")
        print("\n")
