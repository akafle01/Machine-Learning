import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from google.colab import drive
drive.mount('/content/drive')
file_path = '/content/drive/MyDrive/D3.csv'
sample = pd.DataFrame(pd.read_csv(file_path))
sample.head()


# Define the target variable ('Y' in your case)
target_variable = 'Y'

# Extract explanatory variables (X1, X2, X3)
explanatory_variables = ['X1', 'X2', 'X3']

# Function for gradient descent
def gradient_descent(X, y, learning_rate, num_iterations):
    m, n = X.shape
    theta = np.zeros((n, 1))
    cost_history = []

    for i in range(num_iterations):
        gradients = (1/m) * X.T.dot(X.dot(theta) - y)
        theta -= learning_rate * gradients
        cost = (1/(2*m)) * np.sum((X.dot(theta) - y)**2)
        cost_history.append(cost)

    return theta, cost_history

# Define hyperparameters
learning_rates = [0.1, 0.05, 0.01]  # You can explore different learning rates
num_iterations = 1000

# Initialize dictionaries to store results
results = {}

# Create a combined feature matrix with all explanatory variables
X_combined = sample[explanatory_variables].values
X_combined_b = np.c_[np.ones((X_combined.shape[0], 1)), X_combined]

# Perform linear regression with gradient descent for each learning rate
for learning_rate in learning_rates:
    theta, cost_history = gradient_descent(X_combined_b, sample[target_variable].values.reshape(-1, 1),
                                           learning_rate, num_iterations)

    # Store results for this learning rate
    results[learning_rate] = {"theta": theta, "cost_history": cost_history}

# a. Plot loss over the iteration for each learning rate
plt.figure(figsize=(10, 6))
for learning_rate, result in results.items():
    plt.plot(result["cost_history"], label=f'LR={learning_rate}')

plt.title('Loss over Iterations')
plt.xlabel('Iteration')
plt.ylabel('Loss (Cost)')
plt.legend()
plt.grid()

# b. Describe the impact of different learning rates on the final loss and number of training iterations
for learning_rate, result in results.items():
    final_loss = result["cost_history"][-1]
    num_iterations_used = len(result["cost_history"])
    print(f"Learning Rate: {learning_rate}")
    print(f"Final Loss: {final_loss:.4f}")
    print(f"Number of Iterations: {num_iterations_used}")
    print("\n")

# c. Predict the value of Y for new (X1, X2, X3) values
new_data = np.array([[1, 1, 1], [2, 0, 4], [3, 2, 1]])
new_data_b = np.c_[np.ones((new_data.shape[0], 1)), new_data]
predicted_values = new_data_b.dot(results[0.05]["theta"])  # Using the best-performing learning rate (0.05)

print("c. Predicted Y Values for New (X1, X2, X3) Values:")
for i, values in enumerate(new_data):
    print(f"Inputs: {values}, Predicted Y: {predicted_values[i][0]:.4f}")
